{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQAWithSourcesChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "# explicitly set the API key for embedding\n",
    "import openai\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "\n",
    "openai_api_base = os.environ['OPENAI_API_BASE']\n",
    "azure_development_name = os.environ['AZURE_DEVELOPMENT_NAME']\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model in Azure Openai, and the model used to extract information from vectorstore\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_base=openai_api_base,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=azure_development_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_type='azure',\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# model using to extract information from vectorstore\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=openai_api_base,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=azure_development_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_type='azure',\n",
    "    temperature=0,\n",
    "    max_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pdfminer\"\"\"\n",
    "from langchain.document_loaders import ReadTheDocsLoader, PyMuPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "\n",
    "loader = PyMuPDFLoader(\"upload/Resume_Jesse_Chow.pdf\")\n",
    "raw_documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size=1)\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "resume_retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create job duties \n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "jd = \"\"\"\n",
    "\n",
    "Job duties of this Role:\n",
    "\n",
    "At the core of Accenture's Technology business, you are eager to learn and use that curiosity to solve technology problems through developing, designing and maintaining software products or systems that enable client strategies to improve the way our clients and the world works.\n",
    "\n",
    "You will be using your versatility and experience to create and support technology solutions that meet client requirements from analysis to implementation. The latest SDLC best practices will be applied to continuously improve the quality and efficiency of the Accenture development teams.\n",
    "\n",
    "Come JOIN US if you have:\n",
    "• Hands-on experience in application development and design on web applications that integrate to large scale/ mission critical systems using different technologies\n",
    "• Strong in Web (Responsive UI, Angular, React, JavaScript, HTML, CSS) Development\n",
    "• Knowledge on Redux and Saga is a plus\n",
    "• Interested to explore other technologies integration\n",
    "• Passionate and keen to develop your profession in technical delivery, and strive to deliver the best design, codes and practice\n",
    "• Excellent communication skill and the ability to interact professionally with diverse group of stakeholders, internally and externally\n",
    "• University Degree in Computer Science/Engineering, Information Technology/System, or other relevant disciplines desirable\n",
    "• High proficiency in both verbal and written English and Cantonese (Mandarin is an added advantage)\n",
    "• Dynamic and adaptive to the global collaborative project team environment\n",
    "• Candidates with more related experience can be considered as senior position\"\"\"\n",
    "\n",
    "\n",
    "jd_doc = Document(page_content=jd)\n",
    "split_doc = text_splitter.split_documents([jd_doc])\n",
    "job_db = FAISS.from_documents(split_doc, embeddings)\n",
    "job_retriever = job_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"logging \"\"\"\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"openai\").setLevel(logging.DEBUG) # logging.INFO or logging.DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# memory = ConversationBufferWindowMemory(\n",
    "#     memory_key=\"chat_history\",  # important to align with agent prompt (below)\n",
    "#     k=10,\n",
    "#     return_messages=True\n",
    "# )\n",
    "\n",
    "readonly_memory = ReadOnlySharedMemory(memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"This is a conversation between a candidate and a recruiter. The candidate is applying for the job with resume provided.\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Evaluate the performance of candidate from conversation below. \n",
    "\n",
    "{input}:\n",
    "\"\"\"\n",
    "\n",
    "rating_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"chat_history\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "rating_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=rating_prompt,\n",
    "    verbose=True,\n",
    "    memory=readonly_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "resume_prompt_template = \"\"\"Use the following pieces of context to provide information from the resume of the candidate to help rewrite the question to better assess the candidate.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "job_prompt_template = \"\"\"Use the following pieces of context to provide information of the job roles and duties to help rewrite the question to assess whether the candidate meets requirements included in the context.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "RESUME_PROMPT = PromptTemplate(\n",
    "    template=resume_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "JOB_PROMPT = PromptTemplate(\n",
    "    template=job_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "resume_chain_type_kwargs = {\"prompt\": RESUME_PROMPT}\n",
    "job_chain_type_kwargs = {\"prompt\": JOB_PROMPT}\n",
    "\n",
    "retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=resume_retriever,\n",
    "    chain_type_kwargs=resume_chain_type_kwargs\n",
    ")\n",
    "\n",
    "jd_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=job_retriever,\n",
    "    chain_type_kwargs=job_chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create tool\"\"\"\n",
    "from langchain.agents import Tool\n",
    "\n",
    "tool_desc = \"\"\"Use this tool to create interview question related to skills and experience of the candidate. This tool can also be used for create follow up questions from candidate answers.\"\"\"\n",
    "\n",
    "jd_desc = \"\"\"Use this tool to provide information of the job position which the candidate applied to create interview question. This tool can also be used for create follow up questions from candidate answers.\"\"\"\n",
    "\n",
    "rating_desc = \"\"\"Useful for when you summarize a conversation after interview finished. The input to this tool should be a string. Please summarize whether the candidate is a good fit for the position in the following 5 assessment criteria: 1. Technical ability 2. Leadership skills 3. Interpersonal/team skills 4. Presentation skills 5. Attitude and rate the candidate in a scale of 1 to 5, 1 is lowest and 5 is highest.\"\"\"\n",
    "\n",
    "tools = [Tool(\n",
    "    func=retriever.run,\n",
    "    description=tool_desc,\n",
    "    name=\"resume\"\n",
    "), Tool(\n",
    "    func=jd_retriever.run,\n",
    "    description=jd_desc,\n",
    "    name=\"job_duties\"\n",
    "), Tool(\n",
    "    func=rating_chain.run,\n",
    "    description=rating_desc,\n",
    "    name=\"rating\"\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "conversational_agent = initialize_agent(\n",
    "    tools=tools, \n",
    "    llm=model,\n",
    "    agent='chat-conversational-react-description',\n",
    "    verbose=True,\n",
    "    max_iterations=2,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent    \n",
    "# change to 'generate' to ensure meaningful responses\n",
    "evaluate_agent = initialize_agent(\n",
    "    tools=tools, \n",
    "    llm=model,\n",
    "    agent='zero-shot-react-description',\n",
    "    verbose=True,\n",
    "    max_iterations=2,\n",
    "    early_stopping_method=\"generate\", \n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" modify prompt for interviewing bot\"\"\"\n",
    "\n",
    "sys_msg = \"\"\"You are a friendly interviewer asking question to a candidate who applied for the job position in a job interview. Your job is to find out whether the candidate is suitable for the job position.\n",
    "\n",
    "Instructions:\n",
    "- Count the number of questions you asked\n",
    "- You can only ask 5 questions to the candidate\n",
    "- You should keep count of the number of questions you asked, and should not tell the candidate how many questions you asked\n",
    "- After 5 question, you should summarize the performance of the candidate\n",
    "- You should not ask question that is not related to the job roles and the resume\n",
    "- You should only give 1 sentence feedback to previous answer and ask then ask question\n",
    "- You should not answer any questions from candidate\n",
    "- You must not tell the candidate any information about any assessment criteria and your assessment in the middle of the interview\n",
    "- Start asking candidate to introduce themselves to start the interview, this is the first question you should ask\n",
    "\"\"\"\n",
    "\n",
    "prompt = conversational_agent.agent.create_prompt(\n",
    "    system_message=sys_msg,\n",
    "    tools=tools\n",
    ")\n",
    "conversational_agent.agent.llm_chain.prompt = prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='You are a friendly interviewer asking question to a candidate who applied for the job position in a job interview. Your job is to find out whether the candidate is suitable for the job position.\\n\\nInstructions:\\n- Count the number of questions you asked\\n- You can only ask 5 questions to the candidate\\n- You should keep count of the number of questions you asked, and should not tell the candidate how many questions you asked\\n- After 5 question, you should summarize the performance of the candidate\\n- You should not ask question that is not related to the job roles and the resume\\n- You should only give 1 sentence feedback to previous answer and ask then ask question\\n- You should not answer any questions from candidate\\n- You must not tell the candidate any information about any assessment criteria and your assessment in the middle of the interview\\n- Start asking candidate to introduce themselves to start the interview, this is the first question you should ask\\n', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> resume: Use this tool to create interview question related to skills and experience of the candidate. This tool can also be used for create follow up questions from candidate answers.\\n> job_duties: Use this tool to provide information of the job position which the candidate applied to create interview question. This tool can also be used for create follow up questions from candidate answers.\\n> rating: Useful for when you summarize a conversation after interview finished. The input to this tool should be a string. Please summarize whether the candidate is a good fit for the position in the following 5 assessment criteria: 1. Technical ability 2. Leadership skills 3. Interpersonal/team skills 4. Presentation skills 5. Attitude and rate the candidate in a scale of 1 to 5, 1 is lowest and 5 is highest.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string \\\\ The action to take. Must be one of resume, job_duties, rating\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.agent.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOLS\n",
      "------\n",
      "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n",
      "\n",
      "> resume: Use this tool to create interview question related to skills and experience of the candidate. This tool can also be used for create follow up questions from candidate answers.\n",
      "> job_duties: Use this tool to provide information of the job position which the candidate applied to create interview question. This tool can also be used for create follow up questions from candidate answers.\n",
      "> rating: Useful for when you summarize a conversation after interview finished. The input to this tool should be a string. Please summarize whether the candidate is a good fit for the position in the following 5 assessment criteria: 1. Technical ability 2. Leadership skills 3. Interpersonal/team skills 4. Presentation skills 5. Attitude and rate the candidate in a scale of 1 to 5, 1 is lowest and 5 is highest.\n",
      "\n",
      "RESPONSE FORMAT INSTRUCTIONS\n",
      "----------------------------\n",
      "\n",
      "When responding to me, please output a response in one of two formats:\n",
      "\n",
      "**Option 1:**\n",
      "Use this if you want the human to use a tool.\n",
      "Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": string \\ The action to take. Must be one of resume, job_duties, rating\n",
      "    \"action_input\": string \\ The input to the action\n",
      "}}\n",
      "```\n",
      "\n",
      "**Option #2:**\n",
      "Use this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": string \\ You should put what you want to return to use here\n",
      "}}\n",
      "```\n",
      "\n",
      "USER'S INPUT\n",
      "--------------------\n",
      "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
      "\n",
      "{input}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    conversational_agent.agent.llm_chain.prompt.messages[2].prompt.template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "Timeout connecting to server",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/connection.py:699\u001b[0m, in \u001b[0;36mAbstractConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretry\u001b[39m.\u001b[39;49mcall_with_retry(\n\u001b[1;32m    700\u001b[0m         \u001b[39mlambda\u001b[39;49;00m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(), \u001b[39mlambda\u001b[39;49;00m error: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisconnect(error)\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mtimeout:\n",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/retry.py:51\u001b[0m, in \u001b[0;36mRetry.call_with_retry\u001b[0;34m(self, do, fail)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retries \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m failures \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retries:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mraise\u001b[39;00m error\n\u001b[1;32m     52\u001b[0m backoff \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backoff\u001b[39m.\u001b[39mcompute(failures)\n",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/retry.py:46\u001b[0m, in \u001b[0;36mRetry.call_with_retry\u001b[0;34m(self, do, fail)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m do()\n\u001b[1;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_supported_errors \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/connection.py:700\u001b[0m, in \u001b[0;36mAbstractConnection.connect.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry\u001b[39m.\u001b[39mcall_with_retry(\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mlambda\u001b[39;00m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(), \u001b[39mlambda\u001b[39;00m error: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisconnect(error)\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mtimeout:\n",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/connection.py:1002\u001b[0m, in \u001b[0;36mConnection._connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1002\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m   1003\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msocket.getaddrinfo returned an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/connection.py:990\u001b[0m, in \u001b[0;36mConnection._connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[39m# connect\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(socket_address)\n\u001b[1;32m    992\u001b[0m \u001b[39m# set the socket_timeout now that we're connected\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m endpoint \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://redis--jbcbh4t.happyhill-5bfa4661.eastus.azurecontainerapps.io\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      5\u001b[0m r \u001b[39m=\u001b[39m redis\u001b[39m.\u001b[39mfrom_url(\u001b[39m\"\u001b[39m\u001b[39mredis://redis.happyhill-5bfa4661.eastus.azurecontainerapps.io:6379\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m r\u001b[39m.\u001b[39;49mping()\n",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/commands/core.py:1205\u001b[0m, in \u001b[0;36mManagementCommands.ping\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mping\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT:\n\u001b[1;32m   1200\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[39m    Ping the Redis server\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \n\u001b[1;32m   1203\u001b[0m \u001b[39m    For more information see https://redis.io/commands/ping\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_command(\u001b[39m\"\u001b[39;49m\u001b[39mPING\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/client.py:1266\u001b[0m, in \u001b[0;36mRedis.execute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m   1264\u001b[0m pool \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection_pool\n\u001b[1;32m   1265\u001b[0m command_name \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 1266\u001b[0m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection \u001b[39mor\u001b[39;00m pool\u001b[39m.\u001b[39;49mget_connection(command_name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m   1268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1269\u001b[0m     \u001b[39mreturn\u001b[39;00m conn\u001b[39m.\u001b[39mretry\u001b[39m.\u001b[39mcall_with_retry(\n\u001b[1;32m   1270\u001b[0m         \u001b[39mlambda\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_command_parse_response(\n\u001b[1;32m   1271\u001b[0m             conn, command_name, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1272\u001b[0m         ),\n\u001b[1;32m   1273\u001b[0m         \u001b[39mlambda\u001b[39;00m error: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[1;32m   1274\u001b[0m     )\n",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/connection.py:1457\u001b[0m, in \u001b[0;36mConnectionPool.get_connection\u001b[0;34m(self, command_name, *keys, **options)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_use_connections\u001b[39m.\u001b[39madd(connection)\n\u001b[1;32m   1455\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1456\u001b[0m     \u001b[39m# ensure this connection is connected to Redis\u001b[39;00m\n\u001b[0;32m-> 1457\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1458\u001b[0m     \u001b[39m# connections that the pool provides should be ready to send\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m     \u001b[39m# a command. if not, the connection was either returned to the\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m     \u001b[39m# pool before all data has been read or the socket has been\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m     \u001b[39m# closed. either way, reconnect and verify everything is good.\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/development/gpt-interview/.venv/lib/python3.11/site-packages/redis/connection.py:703\u001b[0m, in \u001b[0;36mAbstractConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m     sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry\u001b[39m.\u001b[39mcall_with_retry(\n\u001b[1;32m    700\u001b[0m         \u001b[39mlambda\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect(), \u001b[39mlambda\u001b[39;00m error: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisconnect(error)\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mtimeout:\n\u001b[0;32m--> 703\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTimeout connecting to server\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    704\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    705\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_message(e))\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Timeout connecting to server"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "\n",
    "endpoint = 'https://redis--jbcbh4t.happyhill-5bfa4661.eastus.azurecontainerapps.io'\n",
    "\n",
    "r = redis.StrictRedis(host='redis.happyhill-5bfa4661.eastus.azurecontainerapps.io', port=6379)\n",
    "r.ping()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
